"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["268026"],{670127:function(e,n,i){i.r(n),i.d(n,{default:()=>h,frontMatter:()=>o,metadata:()=>s,assets:()=>a,toc:()=>c,contentTitle:()=>d});var s=JSON.parse('{"id":"lakehouse/storages/hdfs","title":"HDFS","description":"This document is used to introduce the parameters required when accessing HDFS. These parameters apply to:","source":"@site/docs/lakehouse/storages/hdfs.md","sourceDirName":"lakehouse/storages","slug":"/lakehouse/storages/hdfs","permalink":"/docs/dev/lakehouse/storages/hdfs","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"HDFS","language":"en"},"sidebar":"docs","previous":{"title":"File System","permalink":"/docs/dev/lakehouse/metastores/filesystem"},"next":{"title":"S3","permalink":"/docs/dev/lakehouse/storages/s3"}}'),t=i("785893"),r=i("250065");let o={title:"HDFS",language:"en"},d=void 0,a={},c=[{value:"Parameter Overview",id:"parameter-overview",level:2},{value:"Authentication Configuration",id:"authentication-configuration",level:3},{value:"Authentication Types",id:"authentication-types",level:4},{value:"Simple Authentication",id:"simple-authentication",level:5},{value:"Kerberos Authentication",id:"kerberos-authentication",level:5},{value:"Configuration Files",id:"configuration-files",level:3},{value:"IO Optimization",id:"io-optimization",level:2},{value:"Hedged Read",id:"hedged-read",level:3},{value:"<code>dfs.client.socket-timeout</code>",id:"dfsclientsocket-timeout",level:3},{value:"Debugging HDFS",id:"debugging-hdfs",level:2},{value:"HDFS Client",id:"hdfs-client",level:3}];function l(e){let n={a:"a",br:"br",code:"code",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"This document is used to introduce the parameters required when accessing HDFS. These parameters apply to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Catalog properties."}),"\n",(0,t.jsx)(n.li,{children:"Table Valued Function properties."}),"\n",(0,t.jsx)(n.li,{children:"Broker Load properties."}),"\n",(0,t.jsx)(n.li,{children:"Export properties."}),"\n",(0,t.jsx)(n.li,{children:"Outfile properties."}),"\n",(0,t.jsx)(n.li,{children:"Backup and restore"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"parameter-overview",children:"Parameter Overview"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Property Name"}),(0,t.jsx)(n.th,{children:"Former Name"}),(0,t.jsx)(n.th,{children:"Description"}),(0,t.jsx)(n.th,{children:"Default Value"}),(0,t.jsx)(n.th,{children:"Required"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hdfs.authentication.type"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hadoop.security.authentication"})}),(0,t.jsxs)(n.td,{children:["Authentication type for accessing HDFS. Supports ",(0,t.jsx)(n.code,{children:"kerberos"})," and ",(0,t.jsx)(n.code,{children:"simple"})]}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"simple"})}),(0,t.jsx)(n.td,{children:"No"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hdfs.authentication.kerberos.principal"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hadoop.kerberos.principal"})}),(0,t.jsxs)(n.td,{children:["Specifies the principal when the authentication type is ",(0,t.jsx)(n.code,{children:"kerberos"})]}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"No"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hdfs.authentication.kerberos.keytab"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hadoop.kerberos.keytab"})}),(0,t.jsxs)(n.td,{children:["Specifies the keytab when the authentication type is ",(0,t.jsx)(n.code,{children:"kerberos"})]}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"No"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hdfs.impersonation.enabled"})}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsxs)(n.td,{children:["If ",(0,t.jsx)(n.code,{children:"true"}),", HDFS impersonation will be enabled. It will use the proxy user configured in ",(0,t.jsx)(n.code,{children:"core-site.xml"})," to proxy the Doris login user to perform HDFS operations"]}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"Not supported yet"})}),(0,t.jsx)(n.td,{children:"-"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hadoop.username"})}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsxs)(n.td,{children:["When the authentication type is ",(0,t.jsx)(n.code,{children:"simple"}),", this user will be used to access HDFS. By default, the Linux system user running the Doris process will be used"]}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"-"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hadoop.config.resources"})}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsxs)(n.td,{children:["Specifies the directory of HDFS-related configuration files (must include ",(0,t.jsx)(n.code,{children:"hdfs-site.xml"})," and ",(0,t.jsx)(n.code,{children:"core-site.xml"}),"), must use a relative path, the default directory is /plugins/hadoop_conf/ under the (FE/BE) deployment directory (can be changed by modifying hadoop_config_dir in fe.conf/be.conf). All FE and BE nodes must configure the same relative path. Example: ",(0,t.jsx)(n.code,{children:"hadoop/conf/core-site.xml,hadoop/conf/hdfs-site.xml"})]}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"-"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"dfs.nameservices"})}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsxs)(n.td,{children:["Manually configure parameters for HDFS high availability clusters. If configured with ",(0,t.jsx)(n.code,{children:"hadoop.config.resources"}),", parameters will be automatically read from ",(0,t.jsx)(n.code,{children:"hdfs-site.xml"}),". Must be used with the following parameters:",(0,t.jsx)(n.br,{}),(0,t.jsx)(n.code,{children:"dfs.ha.namenodes.your-nameservice"}),(0,t.jsx)(n.br,{}),(0,t.jsx)(n.code,{children:"dfs.namenode.rpc-address.your-nameservice.nn1"}),(0,t.jsx)(n.br,{}),(0,t.jsx)(n.code,{children:"dfs.client.failover.proxy.provider"})," etc."]}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"-"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"authentication-configuration",children:"Authentication Configuration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"hdfs.authentication.type"}),": Used to specify the authentication type. Options are ",(0,t.jsx)(n.code,{children:"kerberos"})," or ",(0,t.jsx)(n.code,{children:"simple"}),". If ",(0,t.jsx)(n.code,{children:"kerberos"})," is selected, the system will use Kerberos authentication to interact with HDFS; if ",(0,t.jsx)(n.code,{children:"simple"})," is used, it means no authentication is used, suitable for open HDFS clusters. Choosing kerberos requires configuring the corresponding principal and keytab."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"hdfs.authentication.kerberos.principal"}),": Specifies the Kerberos principal when the authentication type is ",(0,t.jsx)(n.code,{children:"kerberos"}),". A Kerberos principal is a string that uniquely identifies an identity, usually including the service name, hostname, and domain name."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"hdfs.authentication.kerberos.keytab"}),": This parameter specifies the path to the keytab file used for Kerberos authentication. The keytab file is used to store encrypted credentials, allowing the system to authenticate automatically without requiring the user to manually enter a password."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"authentication-types",children:"Authentication Types"}),"\n",(0,t.jsx)(n.p,{children:"HDFS supports two authentication methods:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Kerberos"}),"\n",(0,t.jsx)(n.li,{children:"Simple"}),"\n"]}),"\n",(0,t.jsx)(n.h5,{id:"simple-authentication",children:"Simple Authentication"}),"\n",(0,t.jsx)(n.p,{children:"Simple authentication is suitable for HDFS clusters where Kerberos is not enabled."}),"\n",(0,t.jsx)(n.p,{children:"To use Simple authentication, the following parameter needs to be set:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-plaintext",children:'"hdfs.authentication.type" = "simple"\n'})}),"\n",(0,t.jsxs)(n.p,{children:["In Simple authentication mode, the ",(0,t.jsx)(n.code,{children:"hadoop.username"})," parameter can be used to specify the username. If not specified, the username of the current process will be used by default."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Example:"})}),"\n",(0,t.jsxs)(n.p,{children:["Access HDFS using the ",(0,t.jsx)(n.code,{children:"lakers"})," username"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-plaintext",children:'"hdfs.authentication.type" = "simple",\n"hadoop.username" = "lakers"\n'})}),"\n",(0,t.jsx)(n.p,{children:"Access HDFS using the default system user"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-plaintext",children:'"hdfs.authentication.type" = "simple"\n'})}),"\n",(0,t.jsx)(n.h5,{id:"kerberos-authentication",children:"Kerberos Authentication"}),"\n",(0,t.jsx)(n.p,{children:"Kerberos authentication is suitable for HDFS clusters where Kerberos is enabled."}),"\n",(0,t.jsx)(n.p,{children:"To use Kerberos authentication, the following parameters need to be set:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-plaintext",children:'"hdfs.authentication.type" = "kerberos"\n"hdfs.authentication.kerberos.principal" = "<your_principal>"\n"hdfs.authentication.kerberos.keytab" = "<your_keytab>"\n'})}),"\n",(0,t.jsx)(n.p,{children:"In Kerberos authentication mode, the Kerberos principal and keytab file path need to be set."}),"\n",(0,t.jsxs)(n.p,{children:["Doris will access HDFS with the identity specified by the ",(0,t.jsx)(n.code,{children:"hdfs.authentication.kerberos.principal"})," property, using the keytab specified by the keytab for authentication."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Note:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The keytab file must exist on every FE and BE node, and the path must be the same, and the user running the Doris process must have read permission for the keytab file."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-plaintext",children:'"hdfs.authentication.type" = "kerberos",\n"hdfs.authentication.kerberos.principal" = "hdfs/hadoop@HADOOP.COM",\n"hdfs.authentication.kerberos.keytab" = "/etc/security/keytabs/hdfs.keytab",\n'})}),"\n",(0,t.jsx)(n.h3,{id:"configuration-files",children:"Configuration Files"}),"\n",(0,t.jsxs)(n.p,{children:["Doris supports specifying the directory of HDFS-related configuration files through the ",(0,t.jsx)(n.code,{children:"hadoop.config.resources"})," parameter."]}),"\n",(0,t.jsxs)(n.p,{children:["The configuration file directory must include ",(0,t.jsx)(n.code,{children:"hdfs-site.xml"})," and ",(0,t.jsx)(n.code,{children:"core-site.xml"})," files, the default directory is ",(0,t.jsx)(n.code,{children:"/plugins/hadoop_conf/"})," under the (FE/BE) deployment directory. All FE and BE nodes must configure the same relative path."]}),"\n",(0,t.jsxs)(n.p,{children:["If the configuration file contains the parameters mentioned in the document above, the parameters explicitly configured by the user will be used preferentially. The configuration file can specify multiple files, separated by commas. For example, ",(0,t.jsx)(n.code,{children:"hadoop/conf/core-site.xml,hadoop/conf/hdfs-site.xml"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"io-optimization",children:"IO Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"hedged-read",children:"Hedged Read"}),"\n",(0,t.jsx)(n.p,{children:"In some cases, high HDFS load may cause reading data replicas from HDFS to take longer, slowing down overall query efficiency. HDFS Client provides Hedged Read functionality.\nThis feature can start another read thread to read the same data when a read request exceeds a certain threshold, using whichever result returns first."}),"\n",(0,t.jsx)(n.p,{children:"Note: This feature may increase HDFS cluster load, use with caution."}),"\n",(0,t.jsx)(n.p,{children:"You can enable this feature as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"create catalog regression properties (\n  'type'='hms',\n  'hive.metastore.uris' = 'thrift://172.21.16.47:7004',\n  'dfs.client.hedged.read.threadpool.size' = '128',\n  'dfs.client.hedged.read.threshold.millis' = \"500\"\n);\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"dfs.client.hedged.read.threadpool.size"})," indicates the number of threads for Hedged Read, shared by one HDFS Client. Typically, BE nodes share one HDFS Client for one HDFS cluster."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"dfs.client.hedged.read.threshold.millis"})," is the read threshold in milliseconds. Hedged Read is triggered when a read request exceeds this threshold."]}),"\n",(0,t.jsx)(n.p,{children:"After enabling, you can see related parameters in Query Profile:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"TotalHedgedRead"}),": Number of Hedged Read initiations."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"HedgedReadWins"}),": Number of successful Hedged Reads (initiated and returned faster than original request)"]}),"\n",(0,t.jsx)(n.p,{children:"Note that these values are cumulative for a single HDFS Client, not per query. The same HDFS Client is reused by multiple queries."}),"\n",(0,t.jsx)(n.h3,{id:"dfsclientsocket-timeout",children:(0,t.jsx)(n.code,{children:"dfs.client.socket-timeout"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"dfs.client.socket-timeout"})," is a client configuration parameter in Hadoop HDFS that sets the socket timeout for client connections with DataNode or NameNode when establishing connections or reading data, measured in milliseconds. The default value is typically 60,000 milliseconds."]}),"\n",(0,t.jsxs)(n.p,{children:["Reducing this parameter's value allows clients to timeout faster and retry or switch to other nodes when encountering network delays, slow DataNode responses, or connection issues. This helps reduce waiting time and improve system response time. For example, in some tests, setting ",(0,t.jsx)(n.code,{children:"dfs.client.socket-timeout"})," to a smaller value (like 5000 milliseconds) can quickly detect DataNode delays or failures, avoiding long waits."]}),"\n",(0,t.jsx)(n.p,{children:"Note:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Setting timeout too low may cause frequent timeout errors during network fluctuations or high node load, affecting task stability."}),"\n",(0,t.jsx)(n.li,{children:"It's recommended to adjust this parameter reasonably based on actual network environment and system load to balance response time and system stability."}),"\n",(0,t.jsxs)(n.li,{children:["This parameter should be set in client configuration files (like ",(0,t.jsx)(n.code,{children:"hdfs-site.xml"}),") to ensure clients use correct timeout values when communicating with HDFS."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["In summary, properly configuring ",(0,t.jsx)(n.code,{children:"dfs.client.socket-timeout"})," can improve I/O response time while ensuring system stability and reliability."]}),"\n",(0,t.jsx)(n.h2,{id:"debugging-hdfs",children:"Debugging HDFS"}),"\n",(0,t.jsx)(n.p,{children:"Hadoop environment configuration is complex, and in some cases, connectivity issues or poor access performance may occur. Here are some third-party tools to help users quickly troubleshoot connectivity issues and basic performance problems."}),"\n",(0,t.jsx)(n.h3,{id:"hdfs-client",children:"HDFS Client"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Java: ",(0,t.jsx)(n.a,{href:"https://github.com/morningman/hdfs-client-java",children:"https://github.com/morningman/hdfs-client-java"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["CPP: ",(0,t.jsx)(n.a,{href:"https://github.com/morningman/hdfs-client-cpp",children:"https://github.com/morningman/hdfs-client-cpp"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These two tools can be used to quickly verify HDFS connectivity and read performance. Most of the Hadoop dependencies in these tools are the same as Doris's own Hadoop dependencies, so they can simulate Doris's access to HDFS scenarios to the greatest extent."}),"\n",(0,t.jsx)(n.p,{children:"The Java version accesses HDFS through Java, which can simulate the logic of Doris FE side accessing HDFS."}),"\n",(0,t.jsx)(n.p,{children:"The CPP version accesses HDFS through C++ & libhdfs, which can simulate the logic of Doris BE side accessing HDFS."}),"\n",(0,t.jsx)(n.p,{children:"For specific usage instructions, please refer to the README of each."})]})}function h(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},250065:function(e,n,i){i.d(n,{Z:function(){return d},a:function(){return o}});var s=i(667294);let t={},r=s.createContext(t);function o(e){let n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);